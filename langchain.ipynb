{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "Les **Agents** dans LangChain ouvrent la voie à des systèmes plus dynamiques, capables de **raisonner étape par étape** et d’**interagir avec des outils** pour accomplir des tâches complexes.\n",
    "\n",
    "Contrairement aux chaînes statiques (chains), **⚠️ un agent ne suit pas un chemin prédéfini**. **Il s’appuie sur un LLM** qui décide dynamiquement, à chaque étape, quelle action entreprendre : quel outil utiliser, quelles informations rechercher ou comment poursuivre, en fonction du contexte.\n",
    "\n",
    "Les outils, ou **tools**, sont des fonctions encapsulées que l’agent peut appeler, il peut s'agir de fonctions pour interroger une base de données, de consulter une API, ou d’exécuter un calcul.\n",
    "\n",
    "**Grâce à cette combinaison :**\n",
    "\n",
    "> raisonnement du LLM → proposition d’action → exécution par l’agent → observation → nouveau raisonnement → et ainsi de suite...\n",
    "\n",
    "... un agent LangChain devient un orchestrateur intelligent, capable de résoudre des problèmes ouverts ou de répondre à des requêtes complexes, sans suivre un script rigide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb160db",
   "metadata": {},
   "source": [
    "![Agent](img/agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3075ae",
   "metadata": {},
   "source": [
    "L’agent suit un **schéma itératif** basé sur le pattern **ReAct (Reasoning + Acting)**.  \n",
    "À partir d’une requête, l'agent interagit avec un modèle de langage (LLM) qui raisonne étape par étape (**Thought**) et propose des actions (**Action**) à effectuer à l’aide d’outils disponibles.   \n",
    "L’agent exécute ces actions, collecte les résultats (**Observation**), et les renvoie au LLM pour affiner son raisonnement.   \n",
    "Ce cycle **ReAct** se répète jusqu’à ce que le LLM formule une réponse finale (**Final Answer**), que l’agent retourne à l’utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a13fb",
   "metadata": {},
   "source": [
    "![Hugging Face](img/hugging_face.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du modèle LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un modèle de langage local grâce à **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion à une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d’interagir facilement avec un modèle comme **llama3** déjà téléchargé via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain import hub\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Chargement des clés d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des modèles en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. Agent standard\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce1aa8",
   "metadata": {},
   "source": [
    "Un agent standard permet d’utiliser un modèle de langage avec un ou plusieurs **outils externes**, comme des fonctions Python, pour répondre à une tâche spécifique.  \n",
    "Cet agent **fonctionne sans mémoire** : il ne conserve **aucun historique des interactions précédentes**. Chaque question est traitée de manière indépendante, comme une **requête isolée**.\n",
    "\n",
    "Dans cet exemple, l’agent utilise un outil simple pour répondre à la question « Quelle heure est-il ? », en appelant une fonction qui retourne l’heure actuelle.\n",
    "Son comportement est guidé par un prompt ReAct standard chargé depuis LangChain Hub, qui lui permet de raisonner et de décider quand utiliser un outil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f33b1",
   "metadata": {},
   "source": [
    "### 2.1 Préparation des outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c53956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'outil : retourne l'heure actuelle au format HH:MM\n",
    "def get_current_time(*args, **kwargs):\n",
    "    return datetime.now().strftime(\"%H:%M\")\n",
    "\n",
    "# Liste des outils que l'agent peut utiliser. Chaque outil est exposé au LLM via un nom et une description.\n",
    "# Cela permet au LLM, durant son raisonnement, de décider quel outil appeler en fonction de la tâche à accomplir.\n",
    "# Ici, un seul outil est défini : \"CurrentTime\", qui retourne l'heure actuelle au format HH:MM.\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"CurrentTime\",\n",
    "        func=get_current_time,\n",
    "        description=\"Use this tool to get the current time.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e15c58",
   "metadata": {},
   "source": [
    "### 2.2 Préparation et usage de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c311b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du prompt standard pour le paradigme ReAct depuis LangChain Hub\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Création de l'agent ReAct avec le modèle, les outils et le prompt\n",
    "agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Encapsulation de l’agent dans un exécuteur avec configuration de contrôle\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True    # Affiche les étapes de raisonnement (utile en debug)\n",
    ")\n",
    "\n",
    "# Lancement de l’agent avec une question en entrée\n",
    "response = executor.invoke({\"input\": \"Quelle heure est-il ?\"})\n",
    "\n",
    "display(Markdown(response[\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbeab8",
   "metadata": {},
   "source": [
    "### 🧩 Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bdabb",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Créez un agent capable de faire des conversions de température. Votre agent doit pouvoir répondre à des questions comme :\n",
    "- *“Quelle est la température en Celsius pour 100 Fahrenheit ?”*\n",
    "- *“Convertis 37.5 degrés Celsius en Fahrenheit.”*\n",
    "\n",
    "💡 Utilisez 2 **tools** différents\n",
    "\n",
    "💪🏻 Bonus : Autoriser des entrées plus souples, comme “Convertis 100 F en C” ou “Celsius pour 212°F”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b977196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45e9a8",
   "metadata": {},
   "source": [
    "# 2. Agent conversationnel\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c71cf",
   "metadata": {},
   "source": [
    "Un agent conversationnel est conçu pour gérer un **dialogue continu**, en conservant une **mémoire des échanges précédents**. Contrairement à l’agent standard qui traite chaque requête indépendamment, un agent conversationnel **peut adapter ses réponses en fonction du contexte accumulé dans la conversation**.\n",
    "\n",
    "Ce type d’agent est particulièrement utile pour construire des assistants interactifs, des conseillers ou des systèmes de FAQ qui doivent s’adapter aux intentions de l’utilisateur au fil du temps.\n",
    "\n",
    "LangChain permet d’ajouter une mémoire à un agent grâce à des modules comme `ConversationBufferMemory`, afin que le modèle de langage puisse prendre en compte l’historique des échanges lors de chaque nouvelle interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3486d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération d’un prompt conversationnel React (basé sur ReAct)\n",
    "prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "\n",
    "# Initialisation de la mémoire pour suivre l’historique des échanges\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Création de l'agent ReAct avec le modèle, les outils et le prompt\n",
    "agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Construction d’un exécuteur qui lie l’agent, les outils et la mémoire\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True    # Affiche les étapes de raisonnement (utile en debug)\n",
    ")\n",
    "\n",
    "# Boucle interactive terminale\n",
    "while True:\n",
    "    user_input = input(\"Vous : \")\n",
    "    clear_output(wait=True)                         # Efface l'affichage précédent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requête de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    response = executor.invoke({\"input\": user_input})\n",
    "    display(Markdown(response[\"output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad82676",
   "metadata": {},
   "source": [
    "### 🧩 Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abd6bb",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Reprenez vos travaux sur l'exercice précédent pour y introduire un aspect conversationnel grâce à une boucle de conversation et à la gestion de la mémoire (`ConversationBufferMemory`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
